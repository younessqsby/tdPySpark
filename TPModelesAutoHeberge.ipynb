{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, min, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 12:47:23 WARN Utils: Your hostname, MacBook-Air-de-Aya.local resolves to a loopback address: 127.0.0.1; using 192.168.1.63 instead (on interface en0)\n",
      "24/12/18 12:47:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/18 12:47:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Exercises Modeles AutoHeberge\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiffre d'affaires total : 2710 \n",
      "Produit le plus vendu : Souris (10 unités)\n"
     ]
    }
   ],
   "source": [
    "# --- Exercice 1: Analyse des ventes ---\n",
    "pathCSVVentes = \"/Users/ayaelyaagoubi/Downloads/ventes.csv\"\n",
    "ventes = spark.read.csv(pathCSVVentes, header=True, inferSchema=True)\n",
    "ventes = ventes.withColumn(\"Chiffre_d_affaires\", col(\"Quantité\") * col(\"Prix_unitaire\"))\n",
    "chiffre_affaires_total = ventes.agg(sum(\"Chiffre_d_affaires\").alias(\"Chiffre_affaires_total\")).collect()[0][0]\n",
    "\n",
    "produit_plus_vendu = ventes.groupBy(\"Produit\").sum(\"Quantité\").withColumnRenamed(\"sum(Quantité)\", \"Total_Quantité\") \\\n",
    "    .orderBy(col(\"Total_Quantité\").desc()).first()\n",
    "\n",
    "print(f\"Chiffre d'affaires total : {chiffre_affaires_total} \")\n",
    "print(f\"Produit le plus vendu : {produit_plus_vendu['Produit']} ({produit_plus_vendu['Total_Quantité']} unités)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Âge moyen : 30.4 ans\n",
      "+---------+-----+\n",
      "|    ville|count|\n",
      "+---------+-----+\n",
      "|Marseille|    1|\n",
      "|    Paris|    2|\n",
      "|     Lyon|    2|\n",
      "+---------+-----+\n",
      "\n",
      "Plus jeune utilisateur : Emma (22 ans)\n"
     ]
    }
   ],
   "source": [
    "# --- Exercice 2: Analyse des utilisateurs ---\n",
    "pathJSONUtilisateurs =  \"/Users/ayaelyaagoubi/Downloads/utilisateurs.json\"\n",
    "utilisateurs = spark.read.json(pathJSONUtilisateurs )\n",
    "\n",
    "age_moyen = utilisateurs.select(avg(\"âge\").alias(\"Âge_moyen\")).collect()[0][0]\n",
    "\n",
    "utilisateurs_par_ville = utilisateurs.groupBy(\"ville\").count()\n",
    "\n",
    "plus_jeune_utilisateur = utilisateurs.orderBy(col(\"âge\").asc()).first()\n",
    "\n",
    "print(f\"Âge moyen : {age_moyen} ans\")\n",
    "utilisateurs_par_ville.show()\n",
    "print(f\"Plus jeune utilisateur : {plus_jeune_utilisateur['nom']} ({plus_jeune_utilisateur['âge']} ans)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+-------+\n",
      "|   Nom| Âge|    Ville| Revenu|\n",
      "+------+----+---------+-------+\n",
      "| Alice|25.0|    Paris|50000.0|\n",
      "|   Bob|30.5|     Lyon|40000.0|\n",
      "|Claire|35.0|Marseille|35000.0|\n",
      "| David|40.0| Inconnue|45000.0|\n",
      "+------+----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Exercice 3: Nettoyage de données ---\n",
    "pathCSVClients = \"/Users/ayaelyaagoubi/Downloads/clients.csv\"\n",
    "clients = spark.read.csv(pathCSVClients, header=True, inferSchema=True)\n",
    "\n",
    "age_moyen = clients.select(mean(\"Âge\").alias(\"moyenne_age\")).collect()[0][0]\n",
    "\n",
    "clients = clients.fillna({\"Âge\": age_moyen})\n",
    "\n",
    "clients = clients.fillna({\"Ville\": \"Inconnue\"})\n",
    "\n",
    "clients = clients.filter(col(\"Revenu\").isNotNull())\n",
    "\n",
    "clients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+\n",
      "|   Produit|   Catégorie|Prix|\n",
      "+----------+------------+----+\n",
      "|Ordinateur|Électronique| 800|\n",
      "|     Table|      Bureau| 150|\n",
      "|Imprimante|Électronique| 120|\n",
      "+----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Exercice 4: Produits les plus chers ---\n",
    "pathCSVProduits = \"/Users/ayaelyaagoubi/Downloads/produits.csv\"\n",
    "produits = spark.read.csv(pathCSVProduits, header=True, inferSchema=True)\n",
    "\n",
    "produits_plus_chers = produits.orderBy(col(\"Prix\").desc()).limit(3)\n",
    "\n",
    "produits_plus_chers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|Client|Dépenses_totales|\n",
      "+------+----------------+\n",
      "|   Bob|             200|\n",
      "| Alice|             250|\n",
      "|  Emma|             350|\n",
      "| David|             250|\n",
      "+------+----------------+\n",
      "\n",
      "Client ayant dépensé le plus : Emma (350 €)\n"
     ]
    }
   ],
   "source": [
    "# --- Exercice 5: Analyse des transactions ---\n",
    "pathCSVTransaction= \"/Users/ayaelyaagoubi/Downloads/transactions.csv\"\n",
    "\n",
    "transactions = spark.read.csv(pathCSVTransaction, header=True, inferSchema=True)\n",
    "\n",
    "depenses_totales = transactions.groupBy(\"Client\").sum(\"Montant\").withColumnRenamed(\"sum(Montant)\", \"Dépenses_totales\")\n",
    "\n",
    "client_plus_depense = depenses_totales.orderBy(col(\"Dépenses_totales\").desc()).first()\n",
    "\n",
    "depenses_totales.show()\n",
    "print(f\"Client ayant dépensé le plus : {client_plus_depense['Client']} ({client_plus_depense['Dépenses_totales']} €)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+\n",
      "|   Catégorie|Prix_moyen|Prix_total|\n",
      "+------------+----------+----------+\n",
      "|Électronique|    238.75|       955|\n",
      "|      Bureau|     115.0|       230|\n",
      "+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Exercice 6: Agrégation de données par catégorie ---\n",
    "pathCSVProduits = \"/Users/ayaelyaagoubi/Downloads/produits.csv\"\n",
    "\n",
    "produits = spark.read.csv(pathCSVProduits, header=True, inferSchema=True)\n",
    "\n",
    "statistiques_categorie = produits.groupBy(\"Catégorie\").agg(\n",
    "    avg(\"Prix\").alias(\"Prix_moyen\"),\n",
    "    sum(\"Prix\").alias(\"Prix_total\")\n",
    ")\n",
    "\n",
    "statistiques_categorie.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
